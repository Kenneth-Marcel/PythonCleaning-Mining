{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data in Python\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Why do we need to clean data?\n",
    "\n",
    "*Inconsistent column names\n",
    "*Missing data\n",
    "*Outliers\n",
    "*Duplicate rows\n",
    "*Untidy\n",
    "*Need to process columns\n",
    "*Column types can signal the presence of unexpected data values (i.e. NaN)\n",
    "\n",
    "\n",
    "Assuming file name is 'df' then:\n",
    "    df.columns gives index of column names\n",
    "    df.shape gives the rows , columns of the data\n",
    "    df.info provides important information on the data file.\n",
    "    df.head and df.tail print the first and last 5 rows of the data set."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Frequency counts: \n",
    "You can count the differing types of values in a column:\n",
    "    \n",
    "    df.{column name}.value_counts(dropna=False)\n",
    "    \n",
    "    Alternative method: df['continent'].value_counts(dropna=False)\n",
    "\n",
    "will output all of the continents alongside the number that each continent shows up in the data set.\n",
    "\n",
    "To investigate the first 5 value counts:\n",
    "    \n",
    "    df.continent.value_counts(dropna=False).head()\n",
    "    \n",
    "These are all useful ways to determine what values are missing or of the incorrect data types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
      "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
      "      dtype='object')\n",
      "58    19\n",
      "57    17\n",
      "54    16\n",
      "59    14\n",
      "52    13\n",
      "51    12\n",
      "62    11\n",
      "44    11\n",
      "60    11\n",
      "56    11\n",
      "64    10\n",
      "41    10\n",
      "63     9\n",
      "67     9\n",
      "55     8\n",
      "45     8\n",
      "42     8\n",
      "53     8\n",
      "61     8\n",
      "65     8\n",
      "43     8\n",
      "66     7\n",
      "50     7\n",
      "48     7\n",
      "46     7\n",
      "49     5\n",
      "47     5\n",
      "39     4\n",
      "35     4\n",
      "68     4\n",
      "70     4\n",
      "40     3\n",
      "71     3\n",
      "69     3\n",
      "38     3\n",
      "34     2\n",
      "37     2\n",
      "77     1\n",
      "76     1\n",
      "74     1\n",
      "29     1\n",
      "Name: age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "#print(df.shape)\n",
    "print(df.columns)\n",
    "print(df.age.value_counts(dropna=False))\n",
    "#print(df.tail())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Summary stats: Can be found via df.describe()\n",
    "You can find the Count, Mean, standard deviation, min, 25%, 50% , 75%, and max.\n",
    "\n",
    "Box plot: (matplotlib)\n",
    "df.boxplot(column='population' , by = 'contitent')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tidy Data\n",
    "\n",
    "Principles of TD:\n",
    "    Columns rep separate variables\n",
    "    Rows rep individual observations\n",
    "    Observational units from tables\n",
    "    \n",
    "For data to be tidy it must have:\n",
    "    Each variable as a separate column\n",
    "    Each row as a separate observation\n",
    "   \n",
    "Reshaping your data using melt:\n",
    "Melting data is the process of turning columns of your data into rows of data. \n",
    "\n",
    "pd.melt is the function used to 'melt' a data frame such that variables in a column are now made into rows - this might make it harder to read but easier to plot values. There are two parameters you should be aware of: id_vars and value_vars. The id_vars represent the columns of the data you do not want to melt (i.e., keep it in its current shape), while the value_vars represent the columns you do wish to melt into rows.\n",
    "\n",
    "It might look something like this:\n",
    "\n",
    "pd.melt(frame=df, id_vars = 'name', value_vars= ['variable1', 'variable2'])\n",
    "\n",
    "In this example, the airquality data set needed to be melted such that all the variables change EXCEPT for the Month and Day variables. \n",
    "\n",
    ">airquality_melt = pd.melt(frame=airquality, id_vars = ['Month','Day'])\n",
    ">print(airquality_melt.head())\n",
    "\n",
    "To make sure the columns have the names we want:\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name= 'measurement', value_name='reading')\n",
    "\n",
    "The pd.melt parameter: var_name allows the first variable to be named if given an answer - the same goes for value_name. This prevents the id_var columns from having the default 'variable' and 'value'.\n",
    "\n",
    "Summary: So what did pd.melt actually do?\n",
    "~~ turns each column  into a variable for a row. For example if my Column displayed the amount of trainers that were purshased in a year alongside the index and the amount of socks that were purchased in that year - by melting both 'Shoes' and 'Socks' it will return a data set where each row is indexed with 'shoe' OR 'sock'.\n",
    "\n",
    "Pivot: un-melting data\n",
    "\n",
    "To pivot data is to do the opposite of melting. This means turning a dataset with rows that repeat variables can now be stored in the same column.\n",
    "\n",
    ".pivot_table() has an index parameter which you can use to specify the columns that you don't want pivoted. Two other parameters that you have to specify are columns (the name of the column you want to pivot), and values (the values to be used when the column is pivoted). \n",
    "\n",
    "If we want to pivot airquality_melt:\n",
    "\n",
    "airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')\n",
    "\n",
    "After pivoting airquality_melt in the previous exercise, you didn't quite get back the original DataFrame. What you got back instead was a pandas DataFrame with a hierarchical index (also known as a MultiIndex).\n",
    "\n",
    "A simple way of getting back to the original DataFrame from the pivoted version: reset_index(). Using this function we can get back to the original DataFrame: airquality."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Pivoting duplicate values\n",
    "So far, you've used the .pivot_table() method when there are multiple index values you want to hold constant during a pivot. In the video, Dan showed you how you can also use pivot tables to deal with duplicate values by providing an aggregation function through the aggfunc parameter. Here, you're going to combine both these uses of pivot tables.\n",
    "\n",
    "Let's say your data collection method accidentally duplicated your dataset. Such a dataset, in which each row is duplicated, has been pre-loaded as airquality_dup. In addition, the airquality_melt DataFrame from the previous exercise has been pre-loaded. Explore their shapes in the IPython Shell by accessing their .shape attributes to confirm the duplicate rows present in airquality_dup.\n",
    "\n",
    "You'll see that by using .pivot_table() and the aggfunc parameter, you can not only reshape your data, but also remove duplicates. Finally, you can then flatten the columns of the pivoted DataFrame using .reset_index().\n",
    "\n",
    "NumPy and pandas have been imported as np and pd respectively.\n",
    "\n",
    "\n",
    "Pivot airquality_dup by using .pivot_table() with the rows indexed by 'Month' and 'Day', the columns indexed by 'measurement', and the values populated with 'reading'. Use np.mean for the aggregation function.\n",
    "Print the head of airquality_pivot.\n",
    "Flatten airquality_pivot by resetting its index.\n",
    "Print the head of airquality_pivot and then the original airquality DataFrame to compare their structure.\n",
    "\n",
    "# Pivot table the airquality_dup: airquality_pivot\n",
    "airquality_pivot = airquality_dup.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "\n",
    "# Print the head of airquality_pivot before reset_index\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "# Reset the index of airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Beyond melt and pivot\n",
    "\n",
    "Common problem: columns contain multiple bits of information.\n",
    "\n",
    "~simply adding new columns to a data frame and assigning parts of an already known column \n",
    "\n",
    "\n",
    "https://data.humdata.org/dataset/ebola-cases-2014\n",
    "\n",
    "^ this data is used for the following section.\n",
    "\n",
    "Splitting a column with .split() and .get()\n",
    "Another common way multiple variables are stored in columns is with a delimiter. You'll learn how to deal with such cases in this exercise, using a dataset consisting of Ebola cases and death counts by state and country. It has been pre-loaded into a DataFrame as ebola.\n",
    "\n",
    "Print the columns of ebola in the IPython Shell using ebola.columns. Notice that the data has column names such as Cases_Guinea and Deaths_Guinea. Here, the underscore _ serves as a delimiter between the first part (cases or deaths), and the second part (country).\n",
    "\n",
    "This time, you cannot directly slice the variable by position as in the previous exercise. You now need to use Python's built-in string method called .split(). By default, this method will split a string into parts separated by a space. However, in this case you want it to split by an underscore. You can do this on Cases_Guinea, for example, using Cases_Guinea.split('_'), which returns the list ['Cases', 'Guinea'].\n",
    "\n",
    "The next challenge is to extract the first element of this list and assign it to a type variable, and the second element of the list to a country variable. You can accomplish this by accessing the str attribute of the column and using the .get() method to retrieve the 0 or 1 index, depending on the part you want.\n",
    "\n",
    "\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt['type_country'].str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Concatenating data:\n",
    "Concatenating multiple sets of data may be problematic due to the indexing of rows. e.g. combining a 0,1,2 data set with another 0,1,2 data set means that the indexing will repeat itself and not express the actual amount of rows.\n",
    "\n",
    "This can be done with the parameter: ignoreindex = True in the pd.concat function.\n",
    "To concatenate data sets column-wise you can alter the parameter of axis=0 to axis=1 which will prevent the concatenation of rows and allow for the columns to 'attach'.\n",
    "\n",
    "What if there are hundreds or thousands to upload and concatenate? - Can use 'globbing'.\n",
    "\n",
    "import glob\n",
    "csv_files = glob.glob('*.csv')\n",
    "print(csv_files)\n",
    "\n",
    "Finding files that match a pattern\n",
    "You're now going to practice using the glob module to find all csv files in the workspace. In the next exercise, you'll programmatically load them into DataFrames.\n",
    "\n",
    "As Dan showed you in the video, the glob module has a function called glob that takes a pattern and returns a list of the files in the working directory that match that pattern.\n",
    "\n",
    "For example, if you know the pattern is part_ single digit number .csv, you can write the pattern as 'part_?.csv' (which would match part_1.csv, part_2.csv, part_3.csv, etc.)\n",
    "\n",
    "Similarly, you can find all .csv files with '*.csv', or all parts with 'part_*'. The ? wildcard represents any 1 character, and the * wildcard represents any number of characters.\n",
    "\n",
    "# Import necessary modules\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = 'part_?.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob('*.csv')\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Merging Data:\n",
    "combine disparate datesets based on common columns.\n",
    "\n",
    "pd.merge(left=data1 , right=data2, on=None, left_on='name1', right_on='name2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The 'type' that each variable or value is matters when analysing data. For example it is already clear that plotting two 'str' types will not work as the computer doesn't know that '5' is an actual integer - the dtype for numerical values has to be an int or a float as '5' is still considered a string. This applies to other variables too.\n",
    "\n",
    "Here is a way of turning the endless columns of 'sex' into a dtype of 'category'.\n",
    "\n",
    "df['sex'] = df['sex'].astype('category')\n",
    "\n",
    "Another example:\n",
    "\n",
    "df['variable1'] = pd.to_numeric(df[variable2] , errors = 'coerce')\n",
    "print(df.info()) \n",
    "\n",
    "This turns all the values in this variable into a float dtype which is plottable. Errors = 'coerce' stops this script from returning an error if a non-float/integer is encountered."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "REGULAR EXPRESSIONS\n",
    "\n",
    "To express a simple integer - you can have \n",
    "for 17 = \\d*\n",
    "for $17 = \\$\\d*\n",
    "for $17.00 = \\$\\d*\\.\\d*\n",
    "for $17.89 = \\$\\d*\\.\\d{2}    for 2 decimal places\n",
    "for $17.895 = ^\\$\\d*\\.\\d{2}$  as we know money only has 2 decimal places.\n",
    "\n",
    "Example:\n",
    "Import re\n",
    "pattern = re.compile('\\$\\d*\\.\\d{2}')\n",
    "result = pattern.match('$17.89')\n",
    "\n",
    "Exercise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "\n",
    "# See if the pattern matches\n",
    "result2 = prog.match('1123-456-7890')\n",
    "print(bool(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Write the first pattern\n",
    "pattern1 = bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890'))\n",
    "print(pattern1)\n",
    "\n",
    "# Write the second pattern\n",
    "pattern2 = bool(re.match(pattern='\\$\\d*\\.\\d{2}', string='$123.45'))\n",
    "print(pattern2)\n",
    "\n",
    "# Write the third pattern\n",
    "# Use [A-Z] to match any capital letter followed by \\w* to match \n",
    "# an arbitrary number of alphanumeric characters.\n",
    "pattern3 = bool(re.match(pattern='A\\w*', string='Australia'))\n",
    "print(pattern3)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use functions to clean data:\n",
    "Complex cleaning - \n",
    "    extract number from string\n",
    "    perform tansformation on extracted number\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
